{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLog(csv_file):\n",
    "    log = pd.read_csv(csv_file, header=None)\n",
    "    for i in range(3):\n",
    "        log.iloc[:,i] = log.iloc[:,i].str.replace('/home/jianjian/Nutstore/Self-Driving/Project4_research/IMG', '../IMG_' + csv_file.split('_')[0][3:])\n",
    "        log.iloc[:,i] = log.iloc[:,i].str.replace('/home/jianjian/Nutstore/Self-Driving/Project4/IMG', '../IMG_' + csv_file.split('_')[0][3:])\n",
    "        \n",
    "    log = np.array(log)\n",
    "    return log\n",
    "\n",
    "# Build the list of filenames of images\n",
    "logs = []\n",
    "logs.extend(readLog('../bCenter_driving_log.csv'))\n",
    "logs.extend(readLog('../aCenter_driving_log.csv'))\n",
    "# logs.extend(readLog('../bFast_driving_log.csv'))\n",
    "logs.extend(readLog('../bReverse_driving_log.csv'))\n",
    "# logs.extend(readLog('../aFast_driving_log.csv'))\n",
    "logs.extend(readLog('../aReverse_driving_log.csv'))\n",
    "# logs.extend(readLog('../bRecover_driving_log.csv'))\n",
    "logs.extend(readLog('../bRecover2_driving_log.csv'))\n",
    "logs.extend(readLog('../aRecover_driving_log.csv'))\n",
    "logs = np.array(logs)\n",
    "\n",
    "valid_logs = []\n",
    "valid_logs.extend(readLog('../valid_driving_log.csv'))\n",
    "valid_logs = np.array(valid_logs)\n",
    "\n",
    "test_logs = []\n",
    "test_logs.extend(readLog('../test_driving_log.csv'))\n",
    "test_logs = np.array(test_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Train/ Validation/ Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = logs[:, 0]\n",
    "y_train = logs[:, 3]\n",
    "\n",
    "X_valid = valid_logs[:, 0]\n",
    "y_valid = valid_logs[:, 3]\n",
    "\n",
    "X_test = test_logs[:, 0]\n",
    "y_test = test_logs[:, 3]\n",
    "\n",
    "del logs\n",
    "del test_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulate MEAN and STD for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because training set is too big, I calculated it partly and summed it up\n",
    "mean_sum = 0\n",
    "cal_batch = 500\n",
    "temp_batch = []\n",
    "for ind, file in enumerate(X_train):\n",
    "    img = mpimg.imread(file)\n",
    "    temp_batch.append(img)\n",
    "    if (ind + 1) % cal_batch == 0:\n",
    "        mean_sum += np.sum(temp_batch, axis=0)\n",
    "        temp_batch = []\n",
    "mean = mean_sum / len(X_train)\n",
    "np.save('mean', mean)\n",
    "\n",
    "std = 0\n",
    "temp_batch = []\n",
    "for ind, file in enumerate(X_train):\n",
    "    img = mpimg.imread(file)\n",
    "    temp_batch.append(img)\n",
    "    if (ind + 1) % cal_batch == 0:\n",
    "        std += np.sum(np.square(temp_batch - mean), axis=0) / len(X_train)\n",
    "        temp_batch = []\n",
    "\n",
    "std = np.sqrt(std)\n",
    "np.save('std', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, BatchNormalization, Dropout, Cropping2D, Lambda, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import Sequence\n",
    "from keras import regularizers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda layer for preprocessing\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x-mean)/std, input_shape=(160, 320, 3)))\n",
    "# Cropped part of images\n",
    "model.add(Cropping2D(cropping=((63, 25), (0, 0))))\n",
    "\n",
    "# CNN layers with ReLU and BN\n",
    "model.add(Convolution2D(24,(5,5), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(36,(5,5), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(48,(5,5), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(64,(3,3), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Convolution2D(64,(3,3), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Applied l2 regularization for not overfitting\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(50, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.02)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation=\"relu\",\n",
    "                kernel_regularizer=regularizers.l2(0.02)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Implemented generator\n",
    "class MY_Generator(Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size):\n",
    "        self.image_filenames, self.labels = shuffle(image_filenames, labels)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        return np.array([mpimg.imread(file_name)\n",
    "               for file_name in batch_x]), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "435/435 [==============================] - 40s 92ms/step - loss: 0.8914 - val_loss: 0.3933\n",
      "Epoch 2/4\n",
      "434/435 [============================>.] - ETA: 0s - loss: 0.1321Epoch 2/4\n",
      "435/435 [==============================] - 34s 79ms/step - loss: 0.1320 - val_loss: 0.1334\n",
      "Epoch 3/4\n",
      "435/435 [==============================] - 34s 79ms/step - loss: 0.0385 - val_loss: 0.0956\n",
      "Epoch 4/4\n",
      "434/435 [============================>.] - ETA: 0s - loss: 0.0229Epoch 4/4\n",
      "435/435 [==============================] - 35s 80ms/step - loss: 0.0229 - val_loss: 0.0941\n",
      "Epoch 5/6\n",
      "435/435 [==============================] - 37s 84ms/step - loss: 0.0182 - val_loss: 0.0675\n",
      "Epoch 6/6\n",
      "435/435 [==============================] - 35s 81ms/step - loss: 0.0148 - val_loss: 0.0883\n",
      "Epoch 7/7\n",
      "435/435 [==============================] - 65s 149ms/step - loss: 0.0135 - val_loss: 0.0621\n"
     ]
    }
   ],
   "source": [
    "my_training_batch_generator = MY_Generator(X_train, y_train, 64)\n",
    "my_valid_batch_generator = MY_Generator(X_valid, y_valid, 64)\n",
    "\n",
    "# MSE as loss function\n",
    "# Adam as optimizor\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs=4, shuffle=True, validation_data=my_valid_batch_generator, use_multiprocessing=True, workers=12, max_queue_size=1)\n",
    "\n",
    "try:\n",
    "    model.save('model4.h5')\n",
    "except:\n",
    "    model.save('model_temp4.h5')\n",
    "    \n",
    "model.fit_generator(generator=my_training_batch_generator, initial_epoch=4, epochs=6, shuffle=True, validation_data=my_valid_batch_generator, use_multiprocessing=True, workers=12, max_queue_size=1)\n",
    "\n",
    "try:\n",
    "    model.save('model6.h5')\n",
    "except:\n",
    "    model.save('model_temp6.h5')\n",
    "    \n",
    "model.fit_generator(generator=my_training_batch_generator, initial_epoch=6, epochs=7, shuffle=True, validation_data=my_valid_batch_generator, use_multiprocessing=True, workers=12, max_queue_size=1)\n",
    "\n",
    "try:\n",
    "    model.save('model7.h5')\n",
    "except:\n",
    "    model.save('model_temp7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# model6 is the best model (through practical test)\n",
    "my_test_batch_generator = MY_Generator(X_test, y_test, 64)\n",
    "model = load_model('model6.h5', custom_objects={'mean': mean, 'std': std})\n",
    "test_loss = model.evaluate_generator(my_test_batch_generator, max_queue_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of test set is 0.0604\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss of test set is %0.4f\" % (test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
